OBJECTIVES

-justify, discuss methods chosen IN LIGHT OF OBJECTIVES
-report how it was done, NOT final product


===part I : setting the scene
>toolchain
    =build eBPF
        -BCC (llvm frontend), clang (llvm-bpf), gcc

    =load eBPF
        -libbpf, syscalls

>mutation
    =bytecode vs sourcecode mutation
    
>utilities
    =ebpf-disasm


===part II : implementation designs

>hiding files & PIDs

>mutation algorithm



REFERENCES


CONTENT

 --- intro
Before delving into the design of the proposed solution, it is necessary to discuss the toolchains that can be utilised to implement the proposed design. The available tools are very complex (alike eBPF itself) yet simultaneously rather inflexible and all emcompassing. It is vital to understand the restrictions the proposed solution must exist under before a design can be implemented.

A remark: this projects designs and implements a system that mutates eBPF bytecode rather than sourcecode. This decision was taken for a very wide range of reasons that are summarised in the following list. Note that they will be explored more thoroughly in the implementation results discussion.

1) Compiler unavailability. eBPF compilers are categorised as cross compilers across every major eBPF compiler implementation and Linux distribution. Distributions typically ship with only a native compiler. A common security hardening feature is to uninstall all available compilers, which is performed much more frequently on servers where eBPF is predominantly used. An argument could be made that a system utilising eBPF is more likely to contain an eBPF compiler, however as will soon be explored, not all eBPF toolchains require a compiler. This is especially true for toolchains targetting administrators rather than developers.

2) Noise. It is only possible to ship a static version of a desired compiler due to the uncertainty of a target system. They take up a substantial amount of space which can't easily be concealed by a rootkit. While statfs may be utilised to retrieve disk usage, direct ioctl requests can be made which differ across drivers. Additionally, compilation of eBPF using libbpf takes seconds, which would be very noticeable on startup.




 --- build system
The userspace part of eBPF infrastructure consists predominantly of three separate components. There is first the eBPF backend, responsible for building eBPF programs from sourcecode into eBPF bytecode. The second component is the eBPF frontend which provides an interface to the backend build system. Finally, the third and last component is the loader, which passes the eBPF program to the kernel. As discussed previously, the eBPF verifier and bytecode-to-native JIT compiler both reside inside the kernel and are not part of userspace.

Both the complexity of eBPF and the lack of maturity means that components often have bugs and incomplete features. Additionally, proper modularisation has arguably not been reached yet, with many components frequently taking on more than one role. This ultimately results in a lack of flexibility, which will be a major point of consideration in the design of our deployment system.

 --- --- backend
Two major (read: functioning) backends exist currently: the Low Level Virtual Machine (LLVM) backend and the GNU Compiler Collection (GCC) backend.

[INSERT TABLE COMPARING LLVM VS GCC]

For perceived reliability and maturity the LLVM backend was chosen for this project.


 --- --- frontend
Unlike the GCC which comes with only its terminal 'gcc' utility and its derivatives as a frontend, LLVM has two frontends for eBPF.

BPF Compiler Collection (BCC) is an LLVM frontend that provides a python interface for eBPF. In essence, BCC is a library integrated into python that allows python scripts to easily define, compile and load eBPF programs. eBPF C sources are defined as a string inside a python script. There are numerous problems with BCC for the use case of this project. For one, BCC does not support loading bytecode objects or loading Compile Once - Run Anywhere (CO-RE) enabled eBPF programs. This alone makes it completely unsuitable.

Clang, the C language compiler, is the other available frontend. Unlike BCC, clang supports generation of both bytecode object files and Executable Linkable Format (ELF) files. This modularity is highly desirable as it allows this project to execute its mutation algorithm between the compilation and loading stages. Additionally, the fact that clang does not cover loading of eBPF programs means numerous loading approach can, and will, be explored. Clang compiles C code, which is the language of Linux so there is no alternatives there!

 --- --- loader
Loading of eBPF programs takes place using the bpf() system call. In addition to loading the eBPF programs into the kernel, eBPF programs must also be attached. Depending on the attachment type (kprobe, tracepoint, socket) the system calls involved differ. Utilisation of eBPF maps and CO-RE substantially complicates the process further, with BPF Type Format (BTF) and map file descriptor arrays needing loading into the kernel and the eBPF program respectively.

A remark: It is worth noting here that BTF allows eBPF programs compiled on one kernel to be loaded on a different kernel reliably and safely. Utilising BTF is necessary to meet the goals of this project.

To combat such complexity faced by the end users libbpf was developed. Libbpf takes care of the fine details involved in attaching and loading different kinds of eBPF programs. It also automates the loading of BTF information and the passing of map file descriptors. Unlike raw system calls however, libbpf uses specially sectioned ELF files. This introduces substantial complexity that will be explored later.

Unlike the previous components, two separate designs were produced and partially implemented. One utilising raw syscalls, the other libbpf.


 --- first iteration design

 The initial design expected 
